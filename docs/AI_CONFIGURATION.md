# AI Agent Configuration Guide

This guide explains how to configure the AI Agent using environment variables with the OpenAI SDK and HuggingFace Router.

---

## Quick Start

### 1. Environment Setup

Your `.env` file is already configured:

```bash
# AI Agent Configuration
VITE_AI_USE_LOCAL_HEURISTICS=true
VITE_AI_USE_CLOUD_LLM=true
VITE_AI_PROVIDER=openai
VITE_AI_HF_TOKEN=hf_your_huggingface_token_here
VITE_AI_OPENAI_BASE_URL=https://router.huggingface.co/v1
VITE_AI_OPENAI_MODEL=zai-org/GLM-5:novita
```

### 2. Restart Dev Server

**Important:** Environment variables are loaded at build time.

```bash
# Stop the current server (Ctrl+C)
npm run dev
```

### 3. Test the AI

Press **Ctrl+K** and try:
- "build a hero section"
- "create a pricing table"
- "make a landing page"

---

## Architecture

### OpenAI SDK + HuggingFace Router

We use the **OpenAI JavaScript SDK** with **HuggingFace Router** to access powerful models:

```typescript
import { OpenAI } from 'openai';

const client = new OpenAI({
    baseURL: "https://router.huggingface.co/v1",
    apiKey: process.env.VITE_AI_HF_TOKEN,
    dangerouslyAllowBrowser: true
});

const response = await client.chat.completions.create({
    model: "zai-org/GLM-5:novita",
    messages: [{ role: "user", content: prompt }]
});
```

**Benefits:**
- âœ… Standard OpenAI API interface
- âœ… Access to multiple HuggingFace models
- âœ… Automatic routing and load balancing
- âœ… Free tier available
- âœ… Easy to switch models

---

## Environment Variables

### `VITE_AI_USE_LOCAL_HEURISTICS`

**Type:** `boolean`  
**Default:** `true`  
**Current:** `true`

Enables Tier 1 (instant regex-based processing).

---

### `VITE_AI_USE_CLOUD_LLM`

**Type:** `boolean`  
**Default:** `false`  
**Current:** `true` âœ…

Enables Tier 2 (AI-powered generation with GLM-5).

---

### `VITE_AI_PROVIDER`

**Type:** `string`  
**Options:** `"openai"` | `"ollama"`  
**Current:** `"openai"` âœ…

Provider selection. Use `"openai"` for HuggingFace Router.

---

### `VITE_AI_HF_TOKEN`

**Type:** `string`  
**Current:** `hf_your_huggingface_token_here` âœ…

Your HuggingFace API token.

**How to get:**
1. Go to https://huggingface.co/settings/tokens
2. Create a new token (read access)
3. Copy and paste into `.env`

---

### `VITE_AI_OPENAI_BASE_URL`

**Type:** `string`  
**Current:** `https://router.huggingface.co/v1` âœ…

HuggingFace Router endpoint (OpenAI-compatible).

---

### `VITE_AI_OPENAI_MODEL`

**Type:** `string`  
**Current:** `zai-org/GLM-5:novita` âœ…

The AI model to use. GLM-5 is optimized for code generation.

**Alternative Models:**
```bash
# Qwen 2.5 Coder (excellent for code)
VITE_AI_OPENAI_MODEL=Qwen/Qwen2.5-Coder-32B-Instruct

# Llama 3.1 (general purpose)
VITE_AI_OPENAI_MODEL=meta-llama/Meta-Llama-3.1-70B-Instruct

# DeepSeek Coder (specialized for code)
VITE_AI_OPENAI_MODEL=deepseek-ai/deepseek-coder-33b-instruct
```

---

## How It Works

### 3-Tier Cascading Architecture

```
User Prompt: "build a pricing section"
    â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ TIER 1: Local Heuristics           â”‚
â”‚ â€¢ Checks regex patterns             â”‚
â”‚ â€¢ No match â†’ Pass to Tier 2        â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
    â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ TIER 2: GLM-5 via OpenAI SDK       â”‚
â”‚ â€¢ Calls HuggingFace Router          â”‚
â”‚ â€¢ Generates novel JSON structure    â”‚
â”‚ â€¢ Returns VectraNode elements       â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
    â†“ (if fails)
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ TIER 3: Template Fallback          â”‚
â”‚ â€¢ Pre-built pricing template        â”‚
â”‚ â€¢ Always works                      â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### System Prompt

The AI receives this instruction:

```
You are a web design AI that generates React component structures.

User Request: "build a pricing section"

You must respond with ONLY valid JSON in this exact format:
{
  "rootId": "unique_id",
  "elements": {
    "unique_id": {
      "id": "unique_id",
      "type": "container",
      "name": "Pricing Section",
      "props": {
        "className": "grid grid-cols-3 gap-8 p-20",
        "layoutMode": "grid"
      },
      "children": []
    }
  }
}

Available types: container, text, heading, button, image, hero_geometric, feature_hover, card
Use Tailwind CSS classes.
```

---

## Testing

### Test 1: Local Heuristics (Tier 1)

```
Prompt: "change button color to red"
Expected: Instant update, no API call
Message: "Locally updated X elements to red."
```

### Test 2: GLM-5 AI Generation (Tier 2)

```
Prompt: "create a testimonial section with 3 cards"
Expected: Novel layout generated by AI
Message: "Generated by GLM-5 AI."
Time: 2-5 seconds
```

### Test 3: Template Fallback (Tier 3)

```
Prompt: "build a hero section"
Expected: Pre-built hero template
Message: "Generated Hero Section (Template Fallback)"
```

---

## Switching Models

### Option 1: Different HuggingFace Model

Edit `.env`:
```bash
# Use Qwen instead of GLM-5
VITE_AI_OPENAI_MODEL=Qwen/Qwen2.5-Coder-32B-Instruct
```

### Option 2: Use Ollama (Local)

```bash
# Install Ollama
curl https://ollama.ai/install.sh | sh
ollama pull llama3
ollama serve

# Update .env
VITE_AI_PROVIDER=ollama
VITE_AI_OLLAMA_ENDPOINT=http://localhost:11434/api/generate
```

### Option 3: Use OpenAI Directly

```bash
# Get API key from https://platform.openai.com/api-keys
VITE_AI_OPENAI_BASE_URL=https://api.openai.com/v1
VITE_AI_OPENAI_MODEL=gpt-4
VITE_AI_HF_TOKEN=sk-your-openai-key-here
```

---

## Troubleshooting

### "Cloud AI Failed" Error

**Cause:** Invalid token or network error

**Solution:**
1. Verify token at https://huggingface.co/settings/tokens
2. Check `VITE_AI_HF_TOKEN` in `.env`
3. Restart dev server
4. Check browser console for errors

### CORS Errors

**Cause:** Browser blocking API requests

**Solution:**
We use `dangerouslyAllowBrowser: true` in the OpenAI client config. This is safe for development but should use a backend proxy in production.

### Model Not Available

**Cause:** Model is loading or unavailable

**Solution:**
```bash
# Try a different model
VITE_AI_OPENAI_MODEL=meta-llama/Meta-Llama-3.1-8B-Instruct
```

### Rate Limiting

**Cause:** Free tier limits exceeded

**Solution:**
- Wait a few minutes
- Use a different model
- Upgrade HuggingFace plan
- Switch to Ollama (no limits)

---

## Performance

| Tier | Latency | Cost | Success Rate |
|------|---------|------|--------------|
| Local Heuristics | ~600ms | $0 | ~30% |
| GLM-5 (Cloud) | 2-5s | Free tier | ~85% |
| Templates | ~1200ms | $0 | 100% |

---

## Security

### Browser Usage

We use `dangerouslyAllowBrowser: true` for development. For production:

**Option 1: Backend Proxy**
```typescript
// Don't expose API key in browser
const response = await fetch('/api/ai', {
    method: 'POST',
    body: JSON.stringify({ prompt })
});
```

**Option 2: Serverless Function**
```typescript
// Vercel/Netlify function
export default async function handler(req, res) {
    const client = new OpenAI({ apiKey: process.env.HF_TOKEN });
    const result = await client.chat.completions.create({...});
    res.json(result);
}
```

### API Key Safety

âœ… `.env` is in `.gitignore`  
âœ… Never commit real tokens  
âœ… Use `.env.example` for templates  
âœ… Rotate keys regularly  

---

## Advanced Usage

### Custom System Prompts

Edit `src/services/aiAgent.ts`:

```typescript
const systemPrompt = `You are a ${brandName} design AI.
Generate components in ${designStyle} style.
Use ${colorScheme} colors.`;
```

### Streaming Responses

```typescript
const stream = await client.chat.completions.create({
    model: AI_CONFIG.openaiModel,
    messages: [...],
    stream: true
});

for await (const chunk of stream) {
    console.log(chunk.choices[0]?.delta?.content);
}
```

### Context Awareness

```typescript
const context = {
    currentElements: Object.keys(elements).length,
    theme: 'dark',
    selectedElement: selectedId
};

const systemPrompt = `Current state: ${JSON.stringify(context)}`;
```

---

## Summary

âœ… **OpenAI SDK** - Standard, well-documented API  
âœ… **HuggingFace Router** - Access to multiple models  
âœ… **GLM-5 Model** - Optimized for code generation  
âœ… **3-Tier Architecture** - Fast, reliable, cost-effective  
âœ… **Browser-Ready** - Works in development immediately  
âœ… **Production-Ready** - Easy to add backend proxy  

Your AI Agent is now powered by GLM-5! ğŸš€

**Next:** Restart your dev server and try: "create a modern pricing section with gradient cards"
